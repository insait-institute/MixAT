default_method_hyperparameters:
  num_test_cases_per_behavior: 1  
  generation_config: # vllm sampling_params: https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py#L22
      temperature: 0.9
      top_p: 0.95
      frequency_penalty: 0.1
      presence_penalty: 0.1
      max_tokens: 256

mixtral_attacker_llm:
  model:
    model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
  num_gpus: 1
#mistralai/Mixtral-8x7B-Instruct-v0.1

zephyr_7b_cat:
  model:    
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: False
    peft_path: ContinuousAT/Zephyr-CAT
    load_in_4bit: True
    dtype: bfloat16
  num_gpus: 1

zephyr_7b:
  model:    
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: False
    load_in_4bit: True
    dtype: bfloat16
  num_gpus: 1